{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward=Variable(torch.zeros(10))\n",
    "forward"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward.expand(10,10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "5"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c=torch.randint(0,5,size=(5,))\n",
    "c.size(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a=torch.randint(0,5,size=(5,5))\n",
    "a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.return_types.max(\nvalues=tensor([4, 4, 4]),\nindices=tensor([2, 2, 2]))"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(a.expand(a.size(0),a.size(0)).t()[:3,:3],dim=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "b=torch.LongTensor([-1]).expand(10)\n",
    "b_=[b]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([1, 3, 4, 4, 2, 0, 3, 1, 1, 2])"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "b_.append(a)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0, 1, 1, 2, 0],\n        [0, 2, 3, 3, 3],\n        [4, 4, 0, 2, 2],\n        [4, 3, 0, 2, 0],\n        [3, 4, 4, 0, 4]])"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.randint(0,5,size=(5,5))\n",
    "a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([1, 0, 0, 2, 4])"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags=[1,0,2,3,1]\n",
    "a[torch.LongTensor(range(c.size(0))),tags]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-1.1032, -0.2201,  0.0417,  ...,  0.8048, -1.7156, -1.6948],\n         [-0.1618,  0.9146, -0.4252,  ..., -0.4482,  0.0149, -0.5817],\n         [-0.8995, -0.4806,  0.6678,  ...,  1.7600, -1.2342, -0.3373],\n         [ 1.4037,  0.1314, -0.3840,  ..., -0.7170,  0.4697, -1.2710],\n         [ 1.3416, -1.4687,  0.3155,  ...,  0.5801, -1.5886,  0.7215]],\n\n        [[-0.6928, -0.6399,  1.3488,  ...,  0.8086,  1.3309, -0.7301],\n         [ 0.7498,  0.7454, -2.2668,  ..., -0.6475, -0.4826,  0.5821],\n         [-0.2068, -0.3305,  1.4889,  ..., -0.6099, -0.6580,  0.3838],\n         [-2.0785, -0.4974, -1.4631,  ..., -0.7480, -0.2731,  0.7450],\n         [-0.5378,  1.6669,  0.7009,  ..., -0.2888,  0.0420,  1.2956]],\n\n        [[ 0.0293,  0.4226,  0.4704,  ..., -1.4362, -1.2979,  2.3129],\n         [-0.8465,  1.3146, -0.3355,  ...,  0.8780, -0.3600,  0.6345],\n         [-0.3634, -0.3572, -0.7092,  ...,  1.3431,  0.3843,  0.9383],\n         [-1.1377,  0.9980,  2.1894,  ..., -0.6741, -0.6323, -1.3789],\n         [ 0.4131,  0.1353, -0.9370,  ...,  0.9664, -0.5555, -0.1728]],\n\n        ...,\n\n        [[ 2.5075,  0.2450,  0.8110,  ...,  0.4288, -0.0257,  0.1454],\n         [ 0.0978,  0.7677, -1.4738,  ..., -0.7951, -1.1810, -0.3533],\n         [ 1.6670, -0.3768, -0.0216,  ...,  0.1485,  0.6998, -0.6106],\n         [ 1.0385,  0.2115, -1.0755,  ...,  0.9270, -0.3577, -0.0077],\n         [ 2.0331,  0.5798,  1.3532,  ..., -0.1674, -0.8866, -0.2691]],\n\n        [[-0.7852,  0.1081,  1.2920,  ..., -0.9753,  0.6887, -0.6812],\n         [-0.7908,  0.0450, -0.1664,  ...,  0.3974,  0.7753,  1.3886],\n         [ 1.0469,  0.0157,  0.0326,  ..., -0.8844,  0.1560, -1.7933],\n         [-1.4987,  0.5967, -0.9768,  ..., -1.5647, -0.8483,  0.3264],\n         [-1.3221,  0.5357,  1.4566,  ..., -0.9642,  0.3696,  0.2090]],\n\n        [[ 0.3387,  0.6902,  0.2698,  ..., -0.6279, -0.1819, -0.1973],\n         [-1.2460,  1.4178,  0.7433,  ..., -0.4070, -1.9206,  0.8638],\n         [-1.9819, -0.2634,  0.4832,  ..., -0.3516,  0.3927,  0.3191],\n         [-0.0446, -0.1815, -0.4164,  ..., -0.2257, -0.6204,  0.3943],\n         [ 0.2684,  1.5132, -0.0812,  ...,  0.3341,  0.4123, -1.0245]]])"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedd=torch.randn(size=(32,5,128))\n",
    "embedd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "conv=nn.Conv1d(in_channels=128,out_channels=127,kernel_size=3)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([32, 3, 127])"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result=conv(embedd.permute(0,2,1))\n",
    "result.permute(0,2,1).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([32, 127, 3])"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Part 2\n",
    "metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import re\n",
    "import itertools"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "tags_actual = [\n",
    "            ['O', 'O', 'O', 'O', 'O', 'O', 'B-PER.NAM', 'I-PER.NAM', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],  # 1个实体\n",
    "            ['O', 'B-PER.NAM', 'I-PER.NAM', 'I-PER.NAM', 'O', 'O', 'B-PER.NAM', 'I-PER.NAM', 'O', 'O']  # 2个实体\n",
    "        ]\n",
    "\n",
    "\n",
    "tags_predict = [\n",
    "            ['O', 'O', 'O', 'O', 'O', 'O', 'B-PER.NAM', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],  # 错1个\n",
    "            ['O', 'B-PER.NAM', 'I-PER.NAM', 'I-PER.NAM', 'O', 'O', 'O', 'O', 'O', 'O']  # 漏1个\n",
    "        ]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "'OOOOOOBIOOOOOOOOBIIOOBIOO'"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_actual = list(itertools.chain(*tags_actual))\n",
    "tags_actual = ''.join(list(map(lambda tag: tag[0], tags_actual)))\n",
    "tags_actual"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "'OOOOOOBOOOOOOOOOBIIOOOOOO'"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_predict = list(itertools.chain(*tags_predict))\n",
    "tags_predict = ''.join(list(map(lambda tag: tag[0], tags_predict)))\n",
    "tags_predict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "regex = re.compile('BI*')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tags_predict)==len(tags_actual)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "['6_8', '16_19', '21_23']"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "3"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "<callable_iterator at 0x1a328336208>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex.finditer(tags_actual)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(6, 8), match='BI'>\n",
      "<re.Match object; span=(16, 19), match='BII'>\n",
      "<re.Match object; span=(21, 23), match='BI'>\n"
     ]
    }
   ],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['6_8', '16_19', '21_23']\n",
      "['6_7', '16_19']\n"
     ]
    }
   ],
   "source": [
    "entity_predict = ['{}_{}'.format(m.start(), m.start() + len(m.group()))\n",
    "                          for m in regex.finditer(tags_predict)]\n",
    "entity_actual = ['{}_{}'.format(m.start(), m.start() + len(m.group()))\n",
    "                         for m in regex.finditer(tags_actual)]\n",
    "\n",
    "print(entity_actual)\n",
    "print(entity_predict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_num = len(set(entity_predict) & set(entity_actual))\n",
    "correct_num"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "3"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_num = len(entity_actual)\n",
    "actual_num"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "0.3333333333333333"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(correct_num) / actual_num if actual_num else 0."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "6.222222222222221"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.var([2,4,8])\n",
    "a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "6.222222222222222"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=np.var([1,3,7])\n",
    "b\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Bert"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from ner.Bert.bertcrf import BertCRF\n",
    "model=BertCRF(bert_model_dir=\"../models/chinese-bert-wwm/\",label_size=12,dropout_rate=0.2)\n",
    "  "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "<generator object Module.named_parameters at 0x0000017BF07BBD48>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.named_parameters()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_model.embeddings.word_embeddings.weight\n",
      "bert_model.embeddings.position_embeddings.weight\n",
      "bert_model.embeddings.token_type_embeddings.weight\n",
      "bert_model.embeddings.LayerNorm.weight\n",
      "bert_model.embeddings.LayerNorm.bias\n",
      "bert_model.encoder.layer.0.attention.self.query.weight\n",
      "bert_model.encoder.layer.0.attention.self.query.bias\n",
      "bert_model.encoder.layer.0.attention.self.key.weight\n",
      "bert_model.encoder.layer.0.attention.self.key.bias\n",
      "bert_model.encoder.layer.0.attention.self.value.weight\n",
      "bert_model.encoder.layer.0.attention.self.value.bias\n",
      "bert_model.encoder.layer.0.attention.output.dense.weight\n",
      "bert_model.encoder.layer.0.attention.output.dense.bias\n",
      "bert_model.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "bert_model.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "bert_model.encoder.layer.0.intermediate.dense.weight\n",
      "bert_model.encoder.layer.0.intermediate.dense.bias\n",
      "bert_model.encoder.layer.0.output.dense.weight\n",
      "bert_model.encoder.layer.0.output.dense.bias\n",
      "bert_model.encoder.layer.0.output.LayerNorm.weight\n",
      "bert_model.encoder.layer.0.output.LayerNorm.bias\n",
      "bert_model.encoder.layer.1.attention.self.query.weight\n",
      "bert_model.encoder.layer.1.attention.self.query.bias\n",
      "bert_model.encoder.layer.1.attention.self.key.weight\n",
      "bert_model.encoder.layer.1.attention.self.key.bias\n",
      "bert_model.encoder.layer.1.attention.self.value.weight\n",
      "bert_model.encoder.layer.1.attention.self.value.bias\n",
      "bert_model.encoder.layer.1.attention.output.dense.weight\n",
      "bert_model.encoder.layer.1.attention.output.dense.bias\n",
      "bert_model.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "bert_model.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "bert_model.encoder.layer.1.intermediate.dense.weight\n",
      "bert_model.encoder.layer.1.intermediate.dense.bias\n",
      "bert_model.encoder.layer.1.output.dense.weight\n",
      "bert_model.encoder.layer.1.output.dense.bias\n",
      "bert_model.encoder.layer.1.output.LayerNorm.weight\n",
      "bert_model.encoder.layer.1.output.LayerNorm.bias\n",
      "bert_model.encoder.layer.2.attention.self.query.weight\n",
      "bert_model.encoder.layer.2.attention.self.query.bias\n",
      "bert_model.encoder.layer.2.attention.self.key.weight\n",
      "bert_model.encoder.layer.2.attention.self.key.bias\n",
      "bert_model.encoder.layer.2.attention.self.value.weight\n",
      "bert_model.encoder.layer.2.attention.self.value.bias\n",
      "bert_model.encoder.layer.2.attention.output.dense.weight\n",
      "bert_model.encoder.layer.2.attention.output.dense.bias\n",
      "bert_model.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "bert_model.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "bert_model.encoder.layer.2.intermediate.dense.weight\n",
      "bert_model.encoder.layer.2.intermediate.dense.bias\n",
      "bert_model.encoder.layer.2.output.dense.weight\n",
      "bert_model.encoder.layer.2.output.dense.bias\n",
      "bert_model.encoder.layer.2.output.LayerNorm.weight\n",
      "bert_model.encoder.layer.2.output.LayerNorm.bias\n",
      "bert_model.encoder.layer.3.attention.self.query.weight\n",
      "bert_model.encoder.layer.3.attention.self.query.bias\n",
      "bert_model.encoder.layer.3.attention.self.key.weight\n",
      "bert_model.encoder.layer.3.attention.self.key.bias\n",
      "bert_model.encoder.layer.3.attention.self.value.weight\n",
      "bert_model.encoder.layer.3.attention.self.value.bias\n",
      "bert_model.encoder.layer.3.attention.output.dense.weight\n",
      "bert_model.encoder.layer.3.attention.output.dense.bias\n",
      "bert_model.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "bert_model.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "bert_model.encoder.layer.3.intermediate.dense.weight\n",
      "bert_model.encoder.layer.3.intermediate.dense.bias\n",
      "bert_model.encoder.layer.3.output.dense.weight\n",
      "bert_model.encoder.layer.3.output.dense.bias\n",
      "bert_model.encoder.layer.3.output.LayerNorm.weight\n",
      "bert_model.encoder.layer.3.output.LayerNorm.bias\n",
      "bert_model.encoder.layer.4.attention.self.query.weight\n",
      "bert_model.encoder.layer.4.attention.self.query.bias\n",
      "bert_model.encoder.layer.4.attention.self.key.weight\n",
      "bert_model.encoder.layer.4.attention.self.key.bias\n",
      "bert_model.encoder.layer.4.attention.self.value.weight\n",
      "bert_model.encoder.layer.4.attention.self.value.bias\n",
      "bert_model.encoder.layer.4.attention.output.dense.weight\n",
      "bert_model.encoder.layer.4.attention.output.dense.bias\n",
      "bert_model.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "bert_model.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "bert_model.encoder.layer.4.intermediate.dense.weight\n",
      "bert_model.encoder.layer.4.intermediate.dense.bias\n",
      "bert_model.encoder.layer.4.output.dense.weight\n",
      "bert_model.encoder.layer.4.output.dense.bias\n",
      "bert_model.encoder.layer.4.output.LayerNorm.weight\n",
      "bert_model.encoder.layer.4.output.LayerNorm.bias\n",
      "bert_model.encoder.layer.5.attention.self.query.weight\n",
      "bert_model.encoder.layer.5.attention.self.query.bias\n",
      "bert_model.encoder.layer.5.attention.self.key.weight\n",
      "bert_model.encoder.layer.5.attention.self.key.bias\n",
      "bert_model.encoder.layer.5.attention.self.value.weight\n",
      "bert_model.encoder.layer.5.attention.self.value.bias\n",
      "bert_model.encoder.layer.5.attention.output.dense.weight\n",
      "bert_model.encoder.layer.5.attention.output.dense.bias\n",
      "bert_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "bert_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "bert_model.encoder.layer.5.intermediate.dense.weight\n",
      "bert_model.encoder.layer.5.intermediate.dense.bias\n",
      "bert_model.encoder.layer.5.output.dense.weight\n",
      "bert_model.encoder.layer.5.output.dense.bias\n",
      "bert_model.encoder.layer.5.output.LayerNorm.weight\n",
      "bert_model.encoder.layer.5.output.LayerNorm.bias\n",
      "bert_model.encoder.layer.6.attention.self.query.weight\n",
      "bert_model.encoder.layer.6.attention.self.query.bias\n",
      "bert_model.encoder.layer.6.attention.self.key.weight\n",
      "bert_model.encoder.layer.6.attention.self.key.bias\n",
      "bert_model.encoder.layer.6.attention.self.value.weight\n",
      "bert_model.encoder.layer.6.attention.self.value.bias\n",
      "bert_model.encoder.layer.6.attention.output.dense.weight\n",
      "bert_model.encoder.layer.6.attention.output.dense.bias\n",
      "bert_model.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "bert_model.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "bert_model.encoder.layer.6.intermediate.dense.weight\n",
      "bert_model.encoder.layer.6.intermediate.dense.bias\n",
      "bert_model.encoder.layer.6.output.dense.weight\n",
      "bert_model.encoder.layer.6.output.dense.bias\n",
      "bert_model.encoder.layer.6.output.LayerNorm.weight\n",
      "bert_model.encoder.layer.6.output.LayerNorm.bias\n",
      "bert_model.encoder.layer.7.attention.self.query.weight\n",
      "bert_model.encoder.layer.7.attention.self.query.bias\n",
      "bert_model.encoder.layer.7.attention.self.key.weight\n",
      "bert_model.encoder.layer.7.attention.self.key.bias\n",
      "bert_model.encoder.layer.7.attention.self.value.weight\n",
      "bert_model.encoder.layer.7.attention.self.value.bias\n",
      "bert_model.encoder.layer.7.attention.output.dense.weight\n",
      "bert_model.encoder.layer.7.attention.output.dense.bias\n",
      "bert_model.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "bert_model.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "bert_model.encoder.layer.7.intermediate.dense.weight\n",
      "bert_model.encoder.layer.7.intermediate.dense.bias\n",
      "bert_model.encoder.layer.7.output.dense.weight\n",
      "bert_model.encoder.layer.7.output.dense.bias\n",
      "bert_model.encoder.layer.7.output.LayerNorm.weight\n",
      "bert_model.encoder.layer.7.output.LayerNorm.bias\n",
      "bert_model.encoder.layer.8.attention.self.query.weight\n",
      "bert_model.encoder.layer.8.attention.self.query.bias\n",
      "bert_model.encoder.layer.8.attention.self.key.weight\n",
      "bert_model.encoder.layer.8.attention.self.key.bias\n",
      "bert_model.encoder.layer.8.attention.self.value.weight\n",
      "bert_model.encoder.layer.8.attention.self.value.bias\n",
      "bert_model.encoder.layer.8.attention.output.dense.weight\n",
      "bert_model.encoder.layer.8.attention.output.dense.bias\n",
      "bert_model.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "bert_model.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "bert_model.encoder.layer.8.intermediate.dense.weight\n",
      "bert_model.encoder.layer.8.intermediate.dense.bias\n",
      "bert_model.encoder.layer.8.output.dense.weight\n",
      "bert_model.encoder.layer.8.output.dense.bias\n",
      "bert_model.encoder.layer.8.output.LayerNorm.weight\n",
      "bert_model.encoder.layer.8.output.LayerNorm.bias\n",
      "bert_model.encoder.layer.9.attention.self.query.weight\n",
      "bert_model.encoder.layer.9.attention.self.query.bias\n",
      "bert_model.encoder.layer.9.attention.self.key.weight\n",
      "bert_model.encoder.layer.9.attention.self.key.bias\n",
      "bert_model.encoder.layer.9.attention.self.value.weight\n",
      "bert_model.encoder.layer.9.attention.self.value.bias\n",
      "bert_model.encoder.layer.9.attention.output.dense.weight\n",
      "bert_model.encoder.layer.9.attention.output.dense.bias\n",
      "bert_model.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "bert_model.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "bert_model.encoder.layer.9.intermediate.dense.weight\n",
      "bert_model.encoder.layer.9.intermediate.dense.bias\n",
      "bert_model.encoder.layer.9.output.dense.weight\n",
      "bert_model.encoder.layer.9.output.dense.bias\n",
      "bert_model.encoder.layer.9.output.LayerNorm.weight\n",
      "bert_model.encoder.layer.9.output.LayerNorm.bias\n",
      "bert_model.encoder.layer.10.attention.self.query.weight\n",
      "bert_model.encoder.layer.10.attention.self.query.bias\n",
      "bert_model.encoder.layer.10.attention.self.key.weight\n",
      "bert_model.encoder.layer.10.attention.self.key.bias\n",
      "bert_model.encoder.layer.10.attention.self.value.weight\n",
      "bert_model.encoder.layer.10.attention.self.value.bias\n",
      "bert_model.encoder.layer.10.attention.output.dense.weight\n",
      "bert_model.encoder.layer.10.attention.output.dense.bias\n",
      "bert_model.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "bert_model.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "bert_model.encoder.layer.10.intermediate.dense.weight\n",
      "bert_model.encoder.layer.10.intermediate.dense.bias\n",
      "bert_model.encoder.layer.10.output.dense.weight\n",
      "bert_model.encoder.layer.10.output.dense.bias\n",
      "bert_model.encoder.layer.10.output.LayerNorm.weight\n",
      "bert_model.encoder.layer.10.output.LayerNorm.bias\n",
      "bert_model.encoder.layer.11.attention.self.query.weight\n",
      "bert_model.encoder.layer.11.attention.self.query.bias\n",
      "bert_model.encoder.layer.11.attention.self.key.weight\n",
      "bert_model.encoder.layer.11.attention.self.key.bias\n",
      "bert_model.encoder.layer.11.attention.self.value.weight\n",
      "bert_model.encoder.layer.11.attention.self.value.bias\n",
      "bert_model.encoder.layer.11.attention.output.dense.weight\n",
      "bert_model.encoder.layer.11.attention.output.dense.bias\n",
      "bert_model.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "bert_model.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "bert_model.encoder.layer.11.intermediate.dense.weight\n",
      "bert_model.encoder.layer.11.intermediate.dense.bias\n",
      "bert_model.encoder.layer.11.output.dense.weight\n",
      "bert_model.encoder.layer.11.output.dense.bias\n",
      "bert_model.encoder.layer.11.output.LayerNorm.weight\n",
      "bert_model.encoder.layer.11.output.LayerNorm.bias\n",
      "bert_model.pooler.dense.weight\n",
      "bert_model.pooler.dense.bias\n",
      "linear.weight\n",
      "linear.bias\n",
      "crf.transitions\n"
     ]
    }
   ],
   "source": [
    "for group in model.named_parameters():\n",
    "    print(group[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "tokenizer=BertTokenizer.from_pretrained(\"../models/chinese-bert-wwm/\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': tensor([[ 101, 2769, 1962, 1599, 3614,  872, 1557,  102,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0]])}"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#对文本进行转化\n",
    "texts=\"我 好 喜 欢 你 啊\"\n",
    "encode=tokenizer.encode_plus(texts,max_length=128,padding=\"max_length\",return_tensors=\"pt\",truncation=True)\n",
    "encode"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 101, 2769, 1962, 1599, 3614,  872, 1557,  102,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0]])"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=[]\n",
    "a=encode[\"input_ids\"]\n",
    "a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "[tensor([[ 101, 2769, 1962, 1599, 3614,  872, 1557,  102,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0]]),\n tensor([[ 101, 2769, 1962, 1599, 3614,  872, 1557,  102,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0]])]"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.append(a)\n",
    "b\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 101, 2769, 1962, 1599, 3614,  872, 1557,  102,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0],\n        [ 101, 2769, 1962, 1599, 3614,  872, 1557,  102,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0]])"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(b)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "['x', 'h']"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=[\"我\",\"x\",\"h\",\"n\"]\n",
    "text[1:-1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}